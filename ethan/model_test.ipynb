{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "This code is copied from the [hugging face translation tutorial](https://huggingface.co/docs/transformers/en/tasks/translation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, GenerationConfig\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'What a stroke was this for poor Jane! who would willingly have gone through the world without believing that so much wickedness existed in the whole race of mankind, as was here collected in one individual.',\n",
       "  'fr': 'Quel coup pour la pauvre Jane qui aurait parcouru le monde entier sans s’imaginer qu’il existât dans toute l’humanité autant de noirceur qu’elle en découvrait en ce moment dans un seul homme !'},\n",
       " {'en': \"The ground rose appreciably as it moved away from the sand flats by the waves, and we soon arrived at some long, winding gradients, genuinely steep paths that allowed us to climb little by little; but we had to tread cautiously in the midst of pudding stones that weren't cemented together, and our feet kept skidding on glassy trachyte, made of feldspar and quartz crystals.\",\n",
       "  'fr': \"Le sol s'élevait sensiblement en s'éloignant du relais des flots, et nous Mmes bientôt arrivés à des rampes longues et sinueuses, véritables raidillons qui permettaient de s'élever peu à peu, mais il fallait marcher prudemment au milieu de ces -- conglomérats, qu'aucun ciment ne reliait entre eux, et le pied glissait sur ces trachytes vitreux, faits de cristaux de feldspath et de quartz.\"},\n",
       " {'en': 'But Paris only completed their separation, that Paris which she had desired since her first doll, and where she washed away her provincialism in a week, becoming a woman of fashion at once, and throwing herself into all the luxurious follies of the period.',\n",
       "  'fr': \"Mais Paris devait achever la séparation, ce Paris qu'elle souhaitait depuis sa premiere poupée, et ou elle se lava en huit jours de sa province, élégante d'un coup, jetée a toutes les folies luxueuses de l'époque.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0:3][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_en_to_fr(examples):\n",
    "    source_lang = \"en\"\n",
    "    target_lang = \"fr\"\n",
    "    prefix = \"translate English to French: \"\n",
    "\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_fr_to_en(examples):\n",
    "    source_lang = \"fr\"\n",
    "    target_lang = \"en\"\n",
    "    prefix = \"translate French to English: \"\n",
    "    \n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 101668\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books_en_to_fr = books.map(preprocess_function_en_to_fr, batched=True)\n",
    "tokenized_books_fr_to_en = books.map(preprocess_function_fr_to_en, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(local_model, name: str, num_epochs: int, tokenized_dataset, max_len=32):\n",
    "    local_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=name,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        generation_max_length=max_len, # can reduce this for faster training, but worse preformance (min = 20)\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    local_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=local_training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    local_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = T5ForConditionalGeneration.from_pretrained(\"my_awesome_opus_books_model/checkpoint-12710/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Les légumes partagent leurs ressources avec des bactéries fixatrices d’azote.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "# tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base French to English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model_fr_to_en = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(model_fr_to_en, \"t5_fr_to_en_final\", 2, tokenized_books_fr_to_en)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base English to French Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "\n",
    "model_en_to_fr = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27fa408efe04381b2c1fa913920c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0946, 'grad_norm': 1.380620002746582, 'learning_rate': 1.8426435877261997e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0035, 'grad_norm': 1.4117834568023682, 'learning_rate': 1.6852871754524e-05, 'epoch': 0.16}\n",
      "{'loss': 1.9848, 'grad_norm': 1.416219711303711, 'learning_rate': 1.5279307631785996e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9492, 'grad_norm': 1.4263882637023926, 'learning_rate': 1.3705743509047995e-05, 'epoch': 0.31}\n",
      "{'loss': 1.9197, 'grad_norm': 1.7879889011383057, 'learning_rate': 1.213532651455547e-05, 'epoch': 0.39}\n",
      "{'loss': 1.9166, 'grad_norm': 1.1198539733886719, 'learning_rate': 1.0561762391817467e-05, 'epoch': 0.47}\n",
      "{'loss': 1.9146, 'grad_norm': 1.347274661064148, 'learning_rate': 8.988198269079466e-06, 'epoch': 0.55}\n",
      "{'loss': 1.9027, 'grad_norm': 1.2819278240203857, 'learning_rate': 7.414634146341464e-06, 'epoch': 0.63}\n",
      "{'loss': 1.8993, 'grad_norm': 1.3830379247665405, 'learning_rate': 5.844217151848939e-06, 'epoch': 0.71}\n",
      "{'loss': 1.901, 'grad_norm': 1.3655177354812622, 'learning_rate': 4.270653029110937e-06, 'epoch': 0.79}\n",
      "{'loss': 1.8983, 'grad_norm': 1.0800689458847046, 'learning_rate': 2.697088906372935e-06, 'epoch': 0.87}\n",
      "{'loss': 1.8866, 'grad_norm': 1.1258262395858765, 'learning_rate': 1.1235247836349333e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3968ec9d3b42b0a241593069986f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6593308448791504, 'eval_bleu': 5.224, 'eval_gen_len': 17.6479, 'eval_runtime': 366.7158, 'eval_samples_per_second': 69.31, 'eval_steps_per_second': 4.333, 'epoch': 1.0}\n",
      "{'train_runtime': 976.967, 'train_samples_per_second': 104.065, 'train_steps_per_second': 6.505, 'train_loss': 1.9363052284126672, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "finetune_model(model_en_to_fr, \"t5_en_to_fr_final\", 1, tokenized_books_en_to_fr)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load nessessary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_to_fr_checkpoint = \"checkpoint-6355\"\n",
    "\n",
    "model_en_to_fr = T5ForConditionalGeneration.from_pretrained(f\"t5_en_to_fr/{en_to_fr_checkpoint}/\").to(device)\n",
    "\n",
    "fr_to_en_checkpoint = \"checkpoint-12710\"\n",
    "\n",
    "model_fr_to_en = T5ForConditionalGeneration.from_pretrained(f\"t5_fr_to_en_final/{fr_to_en_checkpoint}/\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Recursive Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Smaller Test and Train Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_len = tokenized_books_en_to_fr[\"test\"].num_rows\n",
    "test_smaller_size = 2500\n",
    "\n",
    "random_indices = np.random.choice(test_dataset_len, test_smaller_size, replace=False)\n",
    "\n",
    "reduced_tokenized_test = tokenized_books_en_to_fr[\"test\"].select(random_indices)\n",
    "\n",
    "# need to do this to prevent random errors ¯\\_(ツ)_/¯\n",
    "test_tokenized_dataset = reduced_tokenized_test.remove_columns(books[\"test\"].column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = tokenized_books_en_to_fr[\"train\"].num_rows\n",
    "smaller_size = 10000 # 10,000\n",
    "\n",
    "random_indices = np.random.choice(dataset_len, smaller_size, replace=False)\n",
    "\n",
    "reduced_tokenized_train = tokenized_books_en_to_fr[\"train\"].select(random_indices)\n",
    "\n",
    "# need to do this to prevent random errors ¯\\_(ツ)_/¯\n",
    "train_tokenized_dataset = reduced_tokenized_train.remove_columns(books[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Recursive Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recursive_data(train_dataset, model_forward = model_en_to_fr, model_reverse = model_fr_to_en, batch_size = 512):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    dataset_list = []\n",
    "    english_sentences = []\n",
    "    french_sentences = []\n",
    "\n",
    "    loader = DataLoader(train_dataset, batch_size = batch_size, collate_fn = data_collator)\n",
    "\n",
    "    for i, tokenized_inputs in tqdm(enumerate(loader), total=len(loader), desc = \"Forward (EN to FR) Inference Pass\"):\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model_forward.generate(\n",
    "                input_ids=tokenized_inputs['input_ids'].to(device),\n",
    "                attention_mask=tokenized_inputs['attention_mask'].to(device),\n",
    "                max_length=128,  # Maximum length for generation\n",
    "                num_return_sequences=1  # Number of sequences to return per input\n",
    "            ).cpu()\n",
    "\n",
    "        decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "        french_sentences.extend(decoded)\n",
    "        decoded = list(map(lambda s: \"translate French to English: \" + s, decoded))\n",
    "\n",
    "        encoded = tokenizer(decoded, return_tensors=\"pt\", max_length=128, padding=True, truncation=True)\n",
    "        dataset_list.append(Dataset.from_dict(encoded))\n",
    "\n",
    "        del generated_outputs, tokenized_inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    reverse_dataset = concatenate_datasets(dataset_list)\n",
    "\n",
    "    loader2 = DataLoader(reverse_dataset, batch_size = batch_size, collate_fn = data_collator)\n",
    "\n",
    "    for i, tokenized_inputs in tqdm(enumerate(loader2), total=len(loader2), desc = \"Reverse (FR to EN) Inference Pass\"):\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model_reverse.generate(\n",
    "                input_ids=tokenized_inputs['input_ids'].to(device),\n",
    "                attention_mask=tokenized_inputs['attention_mask'].to(device),\n",
    "                max_length=128,  # Maximum length for generation\n",
    "                num_return_sequences=1  # Number of sequences to return per input\n",
    "            ).cpu()\n",
    "        decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "        english_sentences.extend(decoded)\n",
    "\n",
    "        del generated_outputs, tokenized_inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    synthetic_data = {\"translation\": []}\n",
    "\n",
    "    for eng, fr in zip(english_sentences, french_sentences):\n",
    "        elem = {\"en\": eng, \"fr\": fr}\n",
    "        synthetic_data[\"translation\"].append(elem)\n",
    "\n",
    "    test_syn_en_to_fr_dataset = Dataset.from_dict(synthetic_data)\n",
    "    test_syn_en_to_fr_dataset = test_syn_en_to_fr_dataset.map(preprocess_function_en_to_fr, batched=True)\n",
    "    test_syn_en_to_fr_dataset = test_syn_en_to_fr_dataset.remove_columns(\"translation\")\n",
    "\n",
    "    return test_syn_en_to_fr_dataset\n",
    "\n",
    "# new_syn_dataset = get_recursive_data(train_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model for Recursive Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_finetune_model(local_model, name: str, num_epochs: int, train_data, test_data):\n",
    "    local_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=name,\n",
    "        eval_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        generation_max_length=32,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    local_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=local_training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    local_trainer.train()\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Loop To Get Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(local_model, test_data, output_dir = \"temp_results\", batch_size = 16):\n",
    "\n",
    "    test_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        generation_max_length=64,\n",
    "        save_total_limit=3,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    test_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=test_training_args,\n",
    "        eval_dataset=test_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    eval_results = test_trainer.evaluate()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af9abe6bb5f4cc783ea89644fe057db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6781498193740845,\n",
       " 'eval_model_preparation_time': 0.0082,\n",
       " 'eval_bleu': 16.6539,\n",
       " 'eval_gen_len': 37.8996,\n",
       " 'eval_runtime': 116.8829,\n",
       " 'eval_samples_per_second': 21.389,\n",
       " 'eval_steps_per_second': 1.343}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loop(model_en_to_fr, test_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_train(model, validate_model, train_data, test_data, num_iters):\n",
    "    blue_score = []\n",
    "    all_scores = []\n",
    "\n",
    "    validate_model.eval()\n",
    "\n",
    "    local_train_data = train_data\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nIteration {i + 1}/{num_iters}\")\n",
    "\n",
    "        rec_finetune_model(model, f\"test_model{i+1}\", 1, local_train_data, test_data)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        local_train_data = get_recursive_data(local_train_data, model, validate_model, batch_size=256)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        eval_metrics = eval_loop(model, test_data)\n",
    "\n",
    "        blue_score.append(eval_metrics[\"eval_bleu\"])\n",
    "        all_scores.append(eval_metrics)\n",
    "    \n",
    "    return blue_score, all_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfd6f534fd94d70946d4151c48e4d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8789, 'grad_norm': 1.4006133079528809, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b183301ecae48a6bb1b6df60f371699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.672284722328186, 'eval_bleu': 16.8545, 'eval_gen_len': 37.8672, 'eval_runtime': 111.3416, 'eval_samples_per_second': 22.453, 'eval_steps_per_second': 1.41, 'epoch': 1.0}\n",
      "{'train_runtime': 173.5248, 'train_samples_per_second': 57.629, 'train_steps_per_second': 3.602, 'train_loss': 1.8758791015625, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb81b0385086478283a4a2c96ab3b59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward (EN to FR) Inference Pass:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bc73d00a294b48b16ead543b2e0616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reverse (FR to EN) Inference Pass:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed64b2d289b84b5eb33ea04f7166a7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8efd42af034ab7bb43587943fde5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bd9d578a444f49880263b9d997a62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.038, 'grad_norm': 1.098865032196045, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7d58e1a8854b71837319eb0878924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7507448196411133, 'eval_bleu': 16.2437, 'eval_gen_len': 38.5548, 'eval_runtime': 113.3231, 'eval_samples_per_second': 22.061, 'eval_steps_per_second': 1.385, 'epoch': 1.0}\n",
      "{'train_runtime': 173.4446, 'train_samples_per_second': 57.655, 'train_steps_per_second': 3.603, 'train_loss': 1.0324531372070314, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d507c4c8624319ab2ea5d40837dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward (EN to FR) Inference Pass:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92824627994749479a4401fe966e13f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reverse (FR to EN) Inference Pass:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55565a89a534a5ca75a81db8401275e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27725d67d8c545c5baaa3e26779f28c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([16.8545, 16.2437],\n",
       " [{'eval_loss': 1.672284722328186,\n",
       "   'eval_model_preparation_time': 0.0004,\n",
       "   'eval_bleu': 16.8545,\n",
       "   'eval_gen_len': 37.8672,\n",
       "   'eval_runtime': 109.7487,\n",
       "   'eval_samples_per_second': 22.779,\n",
       "   'eval_steps_per_second': 1.431},\n",
       "  {'eval_loss': 1.7507448196411133,\n",
       "   'eval_model_preparation_time': 0.003,\n",
       "   'eval_bleu': 16.2437,\n",
       "   'eval_gen_len': 38.5548,\n",
       "   'eval_runtime': 108.9702,\n",
       "   'eval_samples_per_second': 22.942,\n",
       "   'eval_steps_per_second': 1.441}])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_train(model_en_to_fr, model_fr_to_en, train_tokenized_dataset, test_tokenized_dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
