{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "This code is copied from the [hugging face translation tutorial](https://huggingface.co/docs/transformers/en/tasks/translation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, GenerationConfig\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[\"train\"][0:3][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_en_to_fr(examples):\n",
    "    source_lang = \"en\"\n",
    "    target_lang = \"fr\"\n",
    "    prefix = \"translate English to French: \"\n",
    "\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_fr_to_en(examples):\n",
    "    source_lang = \"fr\"\n",
    "    target_lang = \"en\"\n",
    "    prefix = \"translate French to English: \"\n",
    "    \n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books_en_to_fr = books.map(preprocess_function_en_to_fr, batched=True)\n",
    "tokenized_books_fr_to_en = books.map(preprocess_function_fr_to_en, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(local_model, name: str, num_epochs: int, tokenized_dataset, max_len=32):\n",
    "    local_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=name,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        generation_max_length=max_len, # can reduce this for faster training, but worse preformance (min = 20)\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    local_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=local_training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    local_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = T5ForConditionalGeneration.from_pretrained(\"my_awesome_opus_books_model/checkpoint-12710/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "# tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base French to English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "\n",
    "model_fr_to_en = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(model_fr_to_en, \"t5_fr_to_en_final\", 2, tokenized_books_fr_to_en)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base English to French Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "\n",
    "model_en_to_fr = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(model_en_to_fr, \"t5_en_to_fr\", 1, tokenized_books_en_to_fr)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load nessessary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_to_fr_checkpoint = \"checkpoint-6355\"\n",
    "\n",
    "model_en_to_fr = T5ForConditionalGeneration.from_pretrained(f\"t5_en_to_fr/{en_to_fr_checkpoint}/\").to(device)\n",
    "\n",
    "fr_to_en_checkpoint = \"checkpoint-12710\"\n",
    "\n",
    "model_fr_to_en = T5ForConditionalGeneration.from_pretrained(f\"t5_fr_to_en_final/{fr_to_en_checkpoint}/\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Recursive Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Smaller Test and Train Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_len = tokenized_books_en_to_fr[\"test\"].num_rows\n",
    "test_smaller_size = 2500\n",
    "\n",
    "random_indices = np.random.choice(test_dataset_len, test_smaller_size, replace=False)\n",
    "\n",
    "reduced_tokenized_test = tokenized_books_en_to_fr[\"test\"].select(random_indices)\n",
    "\n",
    "# need to do this to prevent random errors ¯\\_(ツ)_/¯\n",
    "test_tokenized_dataset = reduced_tokenized_test.remove_columns(books[\"test\"].column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = tokenized_books_en_to_fr[\"train\"].num_rows\n",
    "smaller_size = 10000 # 10,000\n",
    "\n",
    "random_indices = np.random.choice(dataset_len, smaller_size, replace=False)\n",
    "\n",
    "reduced_tokenized_train = tokenized_books_en_to_fr[\"train\"].select(random_indices)\n",
    "\n",
    "# need to do this to prevent random errors ¯\\_(ツ)_/¯\n",
    "train_tokenized_dataset = reduced_tokenized_train.remove_columns(books[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Recursive Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recursive_data(train_dataset, model_forward = model_en_to_fr, model_reverse = model_fr_to_en, batch_size = 512):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    dataset_list = []\n",
    "    english_sentences = []\n",
    "    french_sentences = []\n",
    "\n",
    "    loader = DataLoader(train_dataset, batch_size = batch_size, collate_fn = data_collator)\n",
    "\n",
    "    for i, tokenized_inputs in tqdm(enumerate(loader), total=len(loader), desc = \"Forward (EN to FR) Inference Pass\"):\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model_forward.generate(\n",
    "                input_ids=tokenized_inputs['input_ids'].to(device),\n",
    "                attention_mask=tokenized_inputs['attention_mask'].to(device),\n",
    "                max_length=128,  # Maximum length for generation\n",
    "                num_return_sequences=1  # Number of sequences to return per input\n",
    "            ).cpu()\n",
    "\n",
    "        decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "        french_sentences.extend(decoded)\n",
    "        decoded = list(map(lambda s: \"translate French to English: \" + s, decoded))\n",
    "\n",
    "        encoded = tokenizer(decoded, return_tensors=\"pt\", max_length=128, padding=True, truncation=True)\n",
    "        dataset_list.append(Dataset.from_dict(encoded))\n",
    "\n",
    "        del generated_outputs, tokenized_inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    reverse_dataset = concatenate_datasets(dataset_list)\n",
    "\n",
    "    loader2 = DataLoader(reverse_dataset, batch_size = batch_size, collate_fn = data_collator)\n",
    "\n",
    "    for i, tokenized_inputs in tqdm(enumerate(loader2), total=len(loader2), desc = \"Reverse (FR to EN) Inference Pass\"):\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model_reverse.generate(\n",
    "                input_ids=tokenized_inputs['input_ids'].to(device),\n",
    "                attention_mask=tokenized_inputs['attention_mask'].to(device),\n",
    "                max_length=128,  # Maximum length for generation\n",
    "                num_return_sequences=1  # Number of sequences to return per input\n",
    "            ).cpu()\n",
    "        decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "        english_sentences.extend(decoded)\n",
    "\n",
    "        del generated_outputs, tokenized_inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    synthetic_data = {\"translation\": []}\n",
    "\n",
    "    for eng, fr in zip(english_sentences, french_sentences):\n",
    "        elem = {\"en\": eng, \"fr\": fr}\n",
    "        synthetic_data[\"translation\"].append(elem)\n",
    "\n",
    "    test_syn_en_to_fr_dataset = Dataset.from_dict(synthetic_data)\n",
    "    test_syn_en_to_fr_dataset = test_syn_en_to_fr_dataset.map(preprocess_function_en_to_fr, batched=True)\n",
    "    test_syn_en_to_fr_dataset = test_syn_en_to_fr_dataset.remove_columns(\"translation\")\n",
    "\n",
    "    return test_syn_en_to_fr_dataset\n",
    "\n",
    "# new_syn_dataset = get_recursive_data(train_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model for Recursive Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_finetune_model(local_model, name: str, num_epochs: int, train_data, test_data):\n",
    "    local_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=name,\n",
    "        eval_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        generation_max_length=32,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    local_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=local_training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    local_trainer.train()\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Loop To Get Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(local_model, test_data, output_dir = \"temp_results\", batch_size = 16, max_len = 64):\n",
    "\n",
    "    test_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        generation_max_length=max_len,\n",
    "        save_total_limit=3,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    test_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=test_training_args,\n",
    "        eval_dataset=test_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    eval_results = test_trainer.evaluate()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_train(model, validate_model, train_data, test_data, num_iters):\n",
    "    blue_score = []\n",
    "    all_scores = []\n",
    "\n",
    "    validate_model.eval()\n",
    "\n",
    "    local_train_data = train_data\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nIteration {i + 1}/{num_iters}\")\n",
    "\n",
    "        rec_finetune_model(model, f\"test_model{i+1}\", 1, local_train_data, test_data)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        local_train_data = get_recursive_data(local_train_data, model, validate_model, batch_size=256)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        eval_metrics = eval_loop(model, test_data)\n",
    "\n",
    "        blue_score.append(eval_metrics[\"eval_bleu\"])\n",
    "        all_scores.append(eval_metrics)\n",
    "    \n",
    "    return blue_score, all_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Recursive Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_train(model_en_to_fr, model_fr_to_en, train_tokenized_dataset, test_tokenized_dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
