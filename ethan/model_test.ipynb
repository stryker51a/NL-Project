{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "This code is copied from the [hugging face translation tutorial](https://huggingface.co/docs/transformers/en/tasks/translation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, GenerationConfig\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'What a stroke was this for poor Jane! who would willingly have gone through the world without believing that so much wickedness existed in the whole race of mankind, as was here collected in one individual.',\n",
       "  'fr': 'Quel coup pour la pauvre Jane qui aurait parcouru le monde entier sans s’imaginer qu’il existât dans toute l’humanité autant de noirceur qu’elle en découvrait en ce moment dans un seul homme !'},\n",
       " {'en': \"The ground rose appreciably as it moved away from the sand flats by the waves, and we soon arrived at some long, winding gradients, genuinely steep paths that allowed us to climb little by little; but we had to tread cautiously in the midst of pudding stones that weren't cemented together, and our feet kept skidding on glassy trachyte, made of feldspar and quartz crystals.\",\n",
       "  'fr': \"Le sol s'élevait sensiblement en s'éloignant du relais des flots, et nous Mmes bientôt arrivés à des rampes longues et sinueuses, véritables raidillons qui permettaient de s'élever peu à peu, mais il fallait marcher prudemment au milieu de ces -- conglomérats, qu'aucun ciment ne reliait entre eux, et le pied glissait sur ces trachytes vitreux, faits de cristaux de feldspath et de quartz.\"},\n",
       " {'en': 'But Paris only completed their separation, that Paris which she had desired since her first doll, and where she washed away her provincialism in a week, becoming a woman of fashion at once, and throwing herself into all the luxurious follies of the period.',\n",
       "  'fr': \"Mais Paris devait achever la séparation, ce Paris qu'elle souhaitait depuis sa premiere poupée, et ou elle se lava en huit jours de sa province, élégante d'un coup, jetée a toutes les folies luxueuses de l'époque.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0:3][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_en_to_fr(examples):\n",
    "    source_lang = \"en\"\n",
    "    target_lang = \"fr\"\n",
    "    prefix = \"translate English to French: \"\n",
    "\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_fr_to_en(examples):\n",
    "    source_lang = \"fr\"\n",
    "    target_lang = \"en\"\n",
    "    prefix = \"translate French to English: \"\n",
    "    \n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 101668\n",
       "})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ed4ab5f41643d2817bb0638c61a953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_books_en_to_fr = books.map(preprocess_function_en_to_fr, batched=True)\n",
    "tokenized_books_fr_to_en = books.map(preprocess_function_fr_to_en, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_opus_books_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books_en_to_fr[\"train\"],\n",
    "    eval_dataset=tokenized_books_en_to_fr[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(local_model, name: str, num_epochs: int, tokenized_dataset):\n",
    "    local_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=name,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True, #change to bf16=True for XPU\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    local_trainer = Seq2SeqTrainer(\n",
    "        model=local_model,\n",
    "        args=local_training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    local_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"my_awesome_opus_books_model/checkpoint-12710/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Les légumes partagent leurs ressources avec des bactéries fixatrices d’azote.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base French to English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model_fr_to_en = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model(model_fr_to_en, \"t5_fr_to_en_final\", 2, tokenized_books_fr_to_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base English to French Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "\n",
    "model_en_to_fr = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27fa408efe04381b2c1fa913920c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0946, 'grad_norm': 1.380620002746582, 'learning_rate': 1.8426435877261997e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0035, 'grad_norm': 1.4117834568023682, 'learning_rate': 1.6852871754524e-05, 'epoch': 0.16}\n",
      "{'loss': 1.9848, 'grad_norm': 1.416219711303711, 'learning_rate': 1.5279307631785996e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9492, 'grad_norm': 1.4263882637023926, 'learning_rate': 1.3705743509047995e-05, 'epoch': 0.31}\n",
      "{'loss': 1.9197, 'grad_norm': 1.7879889011383057, 'learning_rate': 1.213532651455547e-05, 'epoch': 0.39}\n",
      "{'loss': 1.9166, 'grad_norm': 1.1198539733886719, 'learning_rate': 1.0561762391817467e-05, 'epoch': 0.47}\n",
      "{'loss': 1.9146, 'grad_norm': 1.347274661064148, 'learning_rate': 8.988198269079466e-06, 'epoch': 0.55}\n",
      "{'loss': 1.9027, 'grad_norm': 1.2819278240203857, 'learning_rate': 7.414634146341464e-06, 'epoch': 0.63}\n",
      "{'loss': 1.8993, 'grad_norm': 1.3830379247665405, 'learning_rate': 5.844217151848939e-06, 'epoch': 0.71}\n",
      "{'loss': 1.901, 'grad_norm': 1.3655177354812622, 'learning_rate': 4.270653029110937e-06, 'epoch': 0.79}\n",
      "{'loss': 1.8983, 'grad_norm': 1.0800689458847046, 'learning_rate': 2.697088906372935e-06, 'epoch': 0.87}\n",
      "{'loss': 1.8866, 'grad_norm': 1.1258262395858765, 'learning_rate': 1.1235247836349333e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3968ec9d3b42b0a241593069986f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6593308448791504, 'eval_bleu': 5.224, 'eval_gen_len': 17.6479, 'eval_runtime': 366.7158, 'eval_samples_per_second': 69.31, 'eval_steps_per_second': 4.333, 'epoch': 1.0}\n",
      "{'train_runtime': 976.967, 'train_samples_per_second': 104.065, 'train_steps_per_second': 6.505, 'train_loss': 1.9363052284126672, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "finetune_model(model_en_to_fr, \"t5_en_to_fr\", 1, tokenized_books_en_to_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(model, dataset, tokenizer, validate_model, gen_lang=\"en\", inter_lang = \"fr\", batch_size = 16):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    synthetic_data = {\"translation\": {\"en\": [], \"fr\": []}}\n",
    "\n",
    "    gen_lang_msg = \"translate English to French: \"\n",
    "    inter_lang_msg = \"translate French to English: \"\n",
    "    if gen_lang == \"fr\":\n",
    "        gen_lang_msg, inter_lang_msg = inter_lang_msg, gen_lang_msg\n",
    "\n",
    "    tokenized_datasets = dataset.remove_columns(books[\"train\"].column_names)\n",
    "    train_loader = DataLoader(tokenized_datasets, batch_size = batch_size, collate_fn = data_collator)\n",
    "\n",
    "    for i, tokenized_inputs in enumerate(train_loader):\n",
    "        with torch.no_grad():\n",
    "            generated_outputs = model.generate(\n",
    "                input_ids=tokenized_inputs['input_ids'],\n",
    "                attention_mask=tokenized_inputs['attention_mask'],\n",
    "                max_length=128,  # Maximum length for generation\n",
    "                num_return_sequences=1  # Number of sequences to return per input\n",
    "            )\n",
    "        tokenizer.batch_decode(generated_outputs)\n",
    "\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        source_text = gen_lang_msg + dataset[i]['translation'][gen_lang]\n",
    "        inputs = tokenizer(source_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=128)\n",
    "        inter_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        inter_lang_text = inter_lang_msg + inter_translation\n",
    "        inter_inputs = tokenizer(inter_lang_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            final_out = validate_model.generate(**inter_inputs, max_length=128)\n",
    "        gen_lang_translation = tokenizer.decode(final_out, skip_special_tokens=True)\n",
    "\n",
    "        # Add generated synthetic data\n",
    "        synthetic_data[\"translation\"][gen_lang].append(gen_lang_translation)\n",
    "        synthetic_data[\"translation\"][inter_lang].append(inter_translation)\n",
    "\n",
    "    return Dataset.from_dict(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_checkpoint = \"checkpoint-19065\"\n",
    "\n",
    "model_fr_to_en = T5ForConditionalGeneration.from_pretrained(f\"t5_fr_to_en/{local_checkpoint}/\").to(device)\n",
    "# tokenizer_fr_to_en = T5TokenizerFast.from_pretrained(f\"t5_fr_to_en/{local_checkpoint}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate French to English: -- C'est précisément pour ce motif, Sir John, que je mise à deux contre un en sa faveur.\n",
      "\"It is precisely for that reason, Sir John, that I am laying odds of two to one in his favour.\"\n"
     ]
    }
   ],
   "source": [
    "index = 70\n",
    "\n",
    "fr_sent = tokenized_books_fr_to_en[\"test\"][index][\"translation\"]['fr']\n",
    "text = f\"translate French to English: {fr_sent}\"\n",
    "print(text, tokenized_books_fr_to_en[\"test\"][index][\"translation\"]['en'], sep=\"\\n\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"It is precisely for this motif, Sir John, that I put two against one in his favor.\"'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model_fr_to_en.generate(inputs, max_new_tokens=40)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate English to French: What a stroke was this for poor Jane! who would willingly have gone through the world without believing that so much wickedness existed in the whole race of mankind, as was here collected in one individual.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_books_en_to_fr[\"train\"][0][\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What coup for the poor Jane who has wandered the world in a whole world without imagining that it exists in any humanity as noirceur as she finds in this moment in a single man?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text = tokenized_books_en_to_fr[\"train\"][0][\"translation\"]['fr']\n",
    "inputs = tokenizer(source_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "outputs = model_fr_to_en.generate(**inputs, max_length=128)\n",
    "generated_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_train(model, data, validate_model, num_iters):\n",
    "    validate_model.eval()\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        print(f\"\\nIteration {i + 1}/{num_iters}\")\n",
    "\n",
    "        # train model for 1 epoch\n",
    "        finetune_model(model, f\"test_model{i+1}\", 1, data)\n",
    "\n",
    "        synth_data = generate_synthetic_data(model, data[\"train\"], tokenizer, validate_model)\n",
    "\n",
    "        synth_dataset = synth_data.map(preprocess_function_en_to_fr, batched=True)\n",
    "\n",
    "        data[\"train\"] = synth_dataset\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 101668\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_books_fr_to_en[\"train\"][\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TranslationDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataset, source=\"en\", target=\"fr\"):\n",
    "#         self.dataset = dataset\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         input = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenized_books_en_to_fr[\"train\"]\n",
    "\n",
    "tokenized_datasets = tokenized_test.remove_columns(books[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(tokenized_datasets, batch_size = 16, collate_fn = data_collator, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "dataset_list = []\n",
    "english_sentences = []\n",
    "french_sentences = []\n",
    "\n",
    "for i, tokenized_inputs in enumerate(loader):\n",
    "    with torch.no_grad():\n",
    "        generated_outputs = model_en_to_fr.generate(\n",
    "            input_ids=tokenized_inputs['input_ids'].to(device),\n",
    "            attention_mask=tokenized_inputs['attention_mask'].to(device),\n",
    "            max_length=128,  # Maximum length for generation\n",
    "            num_return_sequences=1  # Number of sequences to return per input\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "    english_sentences.extend(decoded)\n",
    "    decoded = list(map(lambda s: \"translate French to English: \" + s, decoded))\n",
    "\n",
    "    encoded = tokenizer(decoded, return_tensors=\"pt\", max_length=128, padding=True, truncation=True)\n",
    "    dataset_list.append(Dataset.from_dict(encoded))\n",
    "    \n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "test_dataset = concatenate_datasets(dataset_list)\n",
    "\n",
    "loader2 = DataLoader(test_dataset, batch_size = 16, collate_fn = data_collator, pin_memory=True)\n",
    "\n",
    "for i, tokenized_inputs in enumerate(loader2):\n",
    "    with torch.no_grad():\n",
    "        generated_outputs = model_fr_to_en.generate(\n",
    "            input_ids=tokenized_inputs['input_ids'].to(device),\n",
    "            attention_mask=tokenized_inputs['attention_mask'].to(device),\n",
    "            max_length=128,  # Maximum length for generation\n",
    "            num_return_sequences=1  # Number of sequences to return per input\n",
    "        )\n",
    "    decoded = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "    french_sentences.extend(decoded)\n",
    "\n",
    "synthetic_data = {\"translation\": []}\n",
    "\n",
    "for eng, fr in zip(english_sentences, french_sentences):\n",
    "    elem = {\"en\": eng, \"fr\": fr}\n",
    "    synthetic_data[\"translation\"].append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': \"Ce fut un coup pour la pauvre Jane! qui aurait volontairement traversé le monde sans croire que l'humanité avait tant de méchantes, comme c'était ici rassemblée dans un seul individu.\",\n",
       "  'fr': 'This was a coup for the poor Jane! who would have deliberately traversed the world without believing that the humanity had tant of minglings, as it had gathered in one individual.'},\n",
       " {'en': \"Le sol s'élevait appréciablement en s'éloignant des plats de sable par les vagues, et nous arrivons bientôt à quelques longs gradients en vent, véritablement abrupts, qui nous permettent de grimper peu à peu; mais nous devions s'en tenir prudentement au milieu de pierres de pudding qui n'étaient pas cédées ensemble, et nos pieds s'enfonçait en trachyte en verre, \",\n",
       "  'fr': 'The soil was a little awry, and we were soon able to reach some long gradients in vent, which we were a little snoring; but we sat prudently in the middle of a stone of pudding which were not coiled together, and our feet were snoring in a trachyte.'},\n",
       " {'en': \"Mais Paris ne termina que leur séparation, celle qu'elle avait souhaitée depuis sa première poupée, et où elle s'éliminait en une semaine son provincialisme, devenant une femme de mode et se jetant dans toutes les luxueuses fléaux de cette époque.\",\n",
       "  'fr': 'But Paris had not ended their separation, which she had wanted since her first pity, and where she dissolved in a week her provincialisme, bringing a woman of mode and stoking in all the luxurious fléaux of this time.'},\n",
       " {'en': \"Venez d'ici le Palais des Tournelles, et interrogez-vous sur l'Abbé de Sainte-Martin, des Tours.\",\n",
       "  'fr': 'Venez visiter le Palais des Tournelles, and ask you about the Abbe de Sainte-Martin, of the Tours.'},\n",
       " {'en': 'He also prit himself the sabre that he jetted to the fare with colère.',\n",
       "  'fr': 'Il se prit aussi de la sabre qu’il jetait au fare avec colère.'},\n",
       " {'en': \"Il devait se grâcer pour ne pas perdre de vue la lumière; seuls lorsque l'enfant passait avec douceur, avec la suppléance d'un serpent, il ne pouvait pas glisser sans brusquer ses membres.\",\n",
       "  'fr': 'He must have been able to save his eyes from the light; only when the child passed with ease, with the suppleance of a serpent, he could not slide without bruising his members.'},\n",
       " {'en': 'Puis, en abaissant encore sa voix, il décrit, en quelques mots amers, son vieux rêve de fraternité.',\n",
       "  'fr': 'Then, abusing his voice, he described, in some words, his old dream of fraternity.'},\n",
       " {'en': \"Il s'enfonça à son oreille, et ajouta (les spectateurs supposaient qu'il recevait sa dernière confession) : «Avez-vous moi, je peux encore vous sauver!»\",\n",
       "  'fr': 'He shook his ears, and added (the spectators assumed he had received his last confession): \"Avez you me, I can still save you!\"'},\n",
       " {'en': \"Je déclare que notre « Bonadventure » pouvait rester ici d'une fin à l'autre, sans même en tirer à son ancre! »\",\n",
       "  'fr': 'I said that our \"Bonadventure\" could remain here for a long time, without even grabbing his head.\"'},\n",
       " {'en': \"Je me sentis ravi tandis que je lui répondis, mais il n'y a pas de froid, ni de pénurie.\",\n",
       "  'fr': 'I was ravined by the reply I made, but he did not have cold, nor a pity.'},\n",
       " {'en': 'Les sources de soufre arrêtèrent parfois leur chemin, et elles devaient les tourner.',\n",
       "  'fr': 'The sources of suppression halted sometimes their way, and they sat on the tourner.'},\n",
       " {'en': \"En fait, d'Artagnan croyait qu'il pouvait être tranquille, l'un de ses deux ennemis étant tué et l'autre consacré à ses intérêts.\",\n",
       "  'fr': 'd’Artagnan croyait, in fact, that he could be tranquil, with one of his two enemies being tue and the other being devoted to his interests.'},\n",
       " {'en': \"Je n'avais pas eu de doutes sur ces choses, que je ne m'en aiit rien à l'idée d'un danger pour moi depuis longtemps : toutes mes craintes étaient enfouies dans les pensées d'une telle pente de brutalité inhumaine, enfermée, et l'horreur de la dégénérescence de la nature humaine, que je n'avais jamais vue auparavant, en\",\n",
       "  'fr': 'I had no doubts about these things, that I had never thought of a danger for me for long: all my craintes were enfused in the thoughts of a tell tale of inhumanity, enfermated, and the horreur of the degénération of the human nature, that I had never seen as a savage.'},\n",
       " {'en': \"Permettez-moi d'entendre que vous reconstruirez l'homme en l'examinant.\",\n",
       "  'fr': 'Permettez-moi d’entendre que vous reconstruirez l’homme en examinant.'},\n",
       " {'en': \"Il l'a frappé une fois, mais comme il l'avait précédé, cette frappe était mortelle; l'épée perdait son cur.\",\n",
       "  'fr': 'He struck him once, but as he had pretended, this struck was mortelle; the sailor perdait his cur.'},\n",
       " {'en': 'Pourquoi ne pas?', 'fr': 'Why not?'},\n",
       " {'en': '\"N\\'assisted in the room any indication that could be made on the trace of the meurtrier?\"',\n",
       "  'fr': '– Ne s’assurait pas dans la chambre d’indications qui pourraient être faites sur la trace du mourrier?'},\n",
       " {'en': 'Les trois premiers étaient sur le visage de Harrison, les deux derniers étaient des comptoirs lourds sur le corps de Wilson.',\n",
       "  'fr': \"The three first were on Harrison's face, the two last were heavy comptoirs on Wilson's body.\"},\n",
       " {'en': '-- Oh! dit-elle.', 'fr': '\"Oh!\" she said.'},\n",
       " {'en': \"Avant que l'on ne s'efforçait de écrire aux autorités autrichiennes pour engager l'arrêter, il ne serait pas possible de trente-six heures ou deux jours.\",\n",
       "  'fr': \"Before the author would not be able to write to the authorities of the trichlors to engage in the arrest, it would not be possible to have thirty-six o'clock or two days.\"},\n",
       " {'en': \"Il me voyait un regard douce et amical, et je l'ai presque accueilli.\",\n",
       "  'fr': 'He saw me a soft and amical look, and I was almost welcomed.'},\n",
       " {'en': \"Pendant les courtes haltes, nous étouffions la chaleur; tandis que nous étions projetés, l'air chaud a presque arrêté mon souffle.\",\n",
       "  'fr': 'During the courtes we sat in the heat; and as we sat, the heat almost stopped my souffle.'},\n",
       " {'en': \"Pendant un instant, il se tenait à l'étourdissement, à la rassurance du front, à la rassurance des limites de son cur.\",\n",
       "  'fr': 'In a moment he stood up to the etourdissement, to the rassurance of the front, to the rassurance of the limits of his curfew.'},\n",
       " {'en': \"Les trois montèrent leurs chevaux, et s'étalaient à un bon rythme, tandis que Porthos promettait à son adversaire de le perforer avec toutes les pressions connues dans les écoles de clôture.\",\n",
       "  'fr': 'The three mounted their horses, and sat at a good pace, and Porthos sat with his opponent of the perforation with all the known pressures in the school of fencing.'},\n",
       " {'en': 'As-tu and I pitié, toi-meme, when I as arraché the poor fille which m’était sicher, at thecadavre of his pere, for the livrer a l’infamie and a honte in ton harem maudit?',\n",
       "  'fr': 'As you and I pitiated, toi-meme, when I arraigned the poor fille which was sure, at thecadaver of his pity, for the booker of the infamie and a honte in a ton of harem maudit?'},\n",
       " {'en': \"Quelques érudettes verts, à l'eau, à l'avant de ces Hôtels somptueux, n'empêchent pas de voir les angles de leurs façades, leurs grandes fenêtres carrées avec des mullions de pierre, leurs porches pointées surchargées de statues, les grandes outlines de leurs murs, toujours claires, et tous ces charmants accidents de l'architecture, qui font que l'art gotique a l\",\n",
       "  'fr': 'Some erudettes vert, to the water, to the entrance of these somptueux Hôtels, do not hinder them to see the angles of their façades, their large windows square with mullions of stone, their porches pointed over statues, the large outline of their murs, always clear, and all these charmants accidents of architecture, which font that the art got.'},\n",
       " {'en': 'Mes yeux étaient tombés sur une belle image, suspendue contre le mur, le portrait de Gräuben.',\n",
       "  'fr': 'My eyes were buried on a beautiful image, suspended against the murmur, the portrait of Gräuben.'},\n",
       " {'en': \"Le petit vapeur, une fois à l'extérieur des piétons, se tourna vers la gauche, et s'épuisant et sourdissant, faisait un point lointain visible par le chagrin du matin.\",\n",
       "  'fr': 'The small steam, once at the outside of the piétons, turned towards the left, and sourdened and sourdened, made a farewell visible by the chagrin of the morning.'},\n",
       " {'en': '– Seule la mienne, avec toute votre pauvreté.',\n",
       "  'fr': '\"Be the mienne, with all your poorness.\"'},\n",
       " {'en': \"-- Non, professeur, mais il faut l'électricité pour s'écouler, des batteries pour produire son électricité, du sodium pour alimenter ses batteries, du charbon pour produire son sodium, et des champs de charbon pour s'enfoncer.\",\n",
       "  'fr': '\"No, professor, but it must be electricity to eat, batteries to produce electricity, sodium to alimente his batteries, carbon to produce sodium, and carbon fields to eat.'},\n",
       " {'en': 'The joy the most vif sembla doubled in the eyes of the Fausta: \"My rival is present,\" said M***, and the fureur of vanity neut plus of bornes.',\n",
       "  'fr': 'La joie la plus vif ressemblait à doubler aux yeux de la Fausta : « Mon rival est présent, dit M. ***, et le fureur de la vanity ne s’établissait plus de bornes.'},\n",
       " {'en': 'Vous ne regardez pas bien.', 'fr': 'You do not see it well.'},\n",
       " {'en': 'This tour Farnèse placated in the beautiful view of the house, which was long enough to be quarantined, large proportions and to complete with columns for trapues, car this room was large and not more than a foot of a solitary inclination.',\n",
       "  'fr': 'Cette visite de Farnèse placée dans la belle vue de la maison, qui était assez longue pour être quarantaine, de grande proportion et à compléter avec des colonnes de trapues, car cette chambre était grande et pas plus d’un pied d’inclinaison solitaire.'},\n",
       " {'en': \"Je ne l'ai pas pire pour cela; au contraire, je me sentis mieux heureux que jamais.\",\n",
       "  'fr': 'I did not want to do so; but I was better pleased than ever.'},\n",
       " {'en': \"Toutes ses petites failles s'éteint avant la poésie de la partie qui l'a absorbée; et, attirée vers cet homme par l'illusion du personnage, elle essaya de se imaginer sa vie — cette vie résonante, extraordinaire, splendide, et cela aurait pu être la sienne si le sort l'avait eu à s'en tenir.\",\n",
       "  'fr': 'All his small frights sat before the spout of the part which absorbs it; and, drawn towards this man by the illusion of the character, she tried to imagine his life - this life resonant, extraordinary, splendid, and it would be the sienne if the sort of thing had been held.'},\n",
       " {'en': 'Le grand César franchit la rivière, et les légendes romaines se campèrent sur ses terres inclinées.',\n",
       "  'fr': 'The Grand César crossed the river, and the légendes of the romanes sat on their sloped land.'},\n",
       " {'en': 'Et, en vérité, nous avons eu une bonne partie de nos munitions vainement.',\n",
       "  'fr': 'And, in truth, we have a good part of our weapons vainly.'},\n",
       " {'en': '-- Ah! quel point il doit être déjà! pensa-t-elle.',\n",
       "  'fr': '\"Ah, what point must he be already?\" she thought.'},\n",
       " {'en': \"-- C'était petit Letty, a-t-il dit-il.\",\n",
       "  'fr': '\"That was little Letty,\" he said.'},\n",
       " {'en': \"Prenez un jour, partagez-le en sections, à chaque section répartisse sa tâche: ne laissez pas de quarts d'heure, de dix minutes, de cinq minutes en chômage en somme, ne comprennent pas tous; faites chaque affaire à son tour avec la méthode, avec une régularité rigide; la journée se terminera presque avant que vous ne soyez conscient de ce qu'il a et vous ne soyez \",\n",
       "  'fr': 'Prepare a day, share them in sections, and each section will return to his task: do not leave quarts of time, 10 minutes, five minutes in a single somme, do not take all; make every case to his tour with the method, with a rigidity; the day is almost before you are aware of what he has done and you are not a sailor.'},\n",
       " {'en': \"La foule les regardait en disparue autour d'un coin de la route; mais Maheude s'écria :\",\n",
       "  'fr': 'The foul looked at them in a swath of a coin of the road; but Maheude cried:'},\n",
       " {'en': \"-- Ne s'échapperait-il pas? répondit l'étranger en tricotant son bras.\",\n",
       "  'fr': '\"Do you escape?\" replied the foreign, tricoting his arms.'},\n",
       " {'en': \"Il n'est pas nécessaire de dire que les graines récoltées par Herbert sur l'île Tabor avaient été soigneusement semées.\",\n",
       "  'fr': 'It is not necessary to say that the grain harvested by Herbert on Tabor Island had been so well smacked.'},\n",
       " {'en': \"Et, d'abord, comment ne vous êtes-vous pas vu depuis deux mois, et que vous vous trouve maintenant dans les places publiques, dans un équipement de bonne qualité!  l'air rouge et jaune, comme une pomme de Caudebec?\",\n",
       "  'fr': 'And, at first, how do you not see you for two months, and that you are now in public places, in a good quality equipment! the red and yellow air, like a Caudebec pomme?'},\n",
       " {'en': \"Il voulait d'abord stocker leur élevage et en domesticuer ceux qu'ils pourraient prendre plus tard.\",\n",
       "  'fr': 'He wanted to stock their eloquentness and domesticate those they could take sooner.'},\n",
       " {'en': '-- Je vais vous écrire, dit Jane, si vous ne soyez pas en mesure de vous affronter.',\n",
       "  'fr': '\"I will write you,\" said Jane, \"if you do not know what to do.'},\n",
       " {'en': 'M. Fogg en a appris tout cela en consultant son Bradshaw, qui lui a donné les mouvements quotidiens des vapeurs transatlantiques.',\n",
       "  'fr': 'M. Fogg learned all that by consulting his Bradshaw, who gave him the daily movements of transatlantic vapors.'},\n",
       " {'en': 'Nous devrions être riés hors du tribunal si nous venions avec une telle histoire et de telles preuves.',\n",
       "  'fr': 'We should be railed to the tribunal if we were to come with a tell story and such evidence.'},\n",
       " {'en': \"-- Dieu, qu'est-ce?\", 'fr': '\"God, what is it?\"'},\n",
       " {'en': '– Voilà pourquoi je suis au centre de l’intimidation et de l’hypocrisie!',\n",
       "  'fr': '\"This is why I am in the centre of intimidation and hypocrisy.\"'},\n",
       " {'en': \"Quand, par une lacune de son argument, il ne réussit pas à convaincre Mathilde de l'innocence des visites de son rival :  ce stade, la fin de la pièce doit être très proche, lui dit-il; c'est une excuse pour moi si je ne peux pas agir mieux.\",\n",
       "  'fr': \"When, by a lacune of his argument, he did not succeed to convince Mathilde of the innocence of his rival's visits: this stade, the fin of the room must be very close, he said; it is a excuse for me if I can not help.\"},\n",
       " {'en': 'Elizabeth accepta leur compagnie, et les trois jeunes femmes se remarquèrent ensemble.',\n",
       "  'fr': 'Elizabeth accepted their company, and the three young women remarquated together.'},\n",
       " {'en': \"En s'enquêtant sur la personne de Comte Norbert, Julien découvrit qu'il portait des bottes et des épis; et je devais porter des chaussures, à l'évidence comme son inférieur.\",\n",
       "  'fr': 'In squishing on the Comte Norbert, Julien discovered that he carried the boots and épis; and I wore the shoes, at the same time as his lower.'},\n",
       " {'en': '-- Je me suis engagé à venir, dit Felton, et je suis venu.',\n",
       "  'fr': '\"I am engaged in coming,\" said Felton, \"and I am going.\"'},\n",
       " {'en': '\"I will not be better for me, pensait-il, I will be chartreux; I suffer less in the rochers of Velleja.',\n",
       "  'fr': '– Je ne serai pas mieux pour moi, pensait-il, je serai charnier ; je souffrirai moins dans les rochers de Velleja.'},\n",
       " {'en': \"Ils étaient tous assis ensemble dans un housse, Bouteloup et les jeunes si fortement sévèrement entraînés entre les boissons que les deux tables ne constituaient qu'une seule.\",\n",
       "  'fr': 'They were all sitting together in a sand, Bouteloup and the young, if solitary, were surrounded by the woods that the two tables were not a single.'},\n",
       " {'en': 'Elle y a pâti, perdue dans les ombres, enterrée, cachée, immuée.',\n",
       "  'fr': 'She was puddled, lost in the shadows, entered, slammed, immuated.'},\n",
       " {'en': \"D'autres instruments, bien entendu à peu près modifiés, ont été fabriqués, des lames pour les plans, les axes, les écloseries, les pièces d'acier à transformer en scies, les chisels, puis du fer pour les bâtons, les cimetières, les clous, etc. Enfin, le 5 mai, la période métallique s'est terminée, les forgeries retournèrent aux Chimneys, et de nouveaux travaux les autorisait \",\n",
       "  'fr': 'Other instruments, well heard in some way modified, were made, ames for the plans, the axes, the ecloseries, the steel pieces to be converted into scies, the chisels, then the fer for the bâtons, the cimetières, the clous, etc. Finally, the 5th, the metal period has been finished, the counterfeits returned to the Chimneys, and new work were done.'},\n",
       " {'en': 'Le chien de Rasseneur avait commencé à écorcer.',\n",
       "  'fr': 'The Rasseneur dog had begun to erupt.'},\n",
       " {'en': '– Comme le prisonnier a fait acte de violence dans l’intérieur de la citadele, lui, en vertu de l’article 157 du règlement, n’aurait-il pas lieu de lui appliquer les menottes pour trois jours?',\n",
       "  'fr': '\"At the time the prisoner was acting violently in the interior of the citadel, he, in the way of the article 157 of the regulation, would not he be able to apply the menottes for three days?\"'},\n",
       " {'en': \"Cette séquence craignante d'événements apparaît dans l'il de mon esprit.\",\n",
       "  'fr': 'This symbiosis of events appeared in my mind.'},\n",
       " {'en': 'Actuellement, en suivant une étroite voie, à peine visible et utilisée uniquement par les chèvres, il se trouvait sur une immense roche, où il pouvait être certain de son isolement total de ses concitoyens.',\n",
       "  'fr': 'Actuellement, following a narrow path, barely visible and used only by the chins, he found himself on an immense rock, where he could be certain of his total isolement of his concitoyens.'},\n",
       " {'en': \"C'est plus que s'il m'avait donné un million.\",\n",
       "  'fr': 'It was more than he had given me a million.'},\n",
       " {'en': \"Lorsque nous étions seuls, mon père s'éleva, et me dit :\",\n",
       "  'fr': 'When we were alone, my father shook, and said:'},\n",
       " {'en': '-- Oui, monsieur.', 'fr': '\"Yes, monsieur.\"'},\n",
       " {'en': \"Phileas Fogg n'avait que vingt-quatre heures de plus pour atteindre Londres; cette durée était nécessaire pour atteindre Liverpool, avec toute la vapeur.\",\n",
       "  'fr': 'Phileas Fogg had not been twenty-four hours to reach London; this time was necessary to reach Liverpool, with all the steam.'},\n",
       " {'en': \"Mais je vais aller plus loin dans mes déductions, et je affirmerai que ce spécimen de la famille humaine est de la race japhétienne, qui s'est depuis élargie des Indes vers l'Atlantique.\",\n",
       "  'fr': 'But I will go farther in my deductions, and I affirm that this speciality of the human family is of the japhtonian race, which has since been alargie of the Indes towards the Atlantic.'},\n",
       " {'en': '-- Oui.', 'fr': '\"Yes.\"'},\n",
       " {'en': 'Sur le devant de la seconde page, la page du titre, il remarquait une sorte de tache qui ressemblait à une encre.',\n",
       "  'fr': 'On the front of the second page, the title page, he remarquated a sort of tache which seemed like a slew of a sleeve.'},\n",
       " {'en': \"C'était une pierre, en arrière-pied, et il y avait une minute ou deux avant de la tirer.\",\n",
       "  'fr': 'It was a stone, in the back of the door, and it was a minute or two before the sand.'},\n",
       " {'en': \"Je s'écriai à la porte humide : je m'écriais : je rongais les mains ; je m'ébranlais en angoisse.\",\n",
       "  'fr': 'I sat down at the humid door: I sat down, I sat down, I sat down, I sat down, I sat down in a sigh.'},\n",
       " {'en': '-- Certainement.', 'fr': '\"Serious.\"'},\n",
       " {'en': 'Vous savez Reims?', 'fr': 'You have Reims?'},\n",
       " {'en': '-- Regardez! voir cette créature malade en demandant alms!',\n",
       "  'fr': '\"Look!\" said the patient, asking alms.'},\n",
       " {'en': \"C'était un feu lourd qui a débordé ces étouffements de mille roches de grêle.\",\n",
       "  'fr': 'It was a heavy feu which had sunk these etouffements of mille rocks of grêle.'},\n",
       " {'en': 'Oh, je viens!', 'fr': 'Oh, I am!'},\n",
       " {'en': \"Pierre, l'abatteur, si vous me regardez comme ça, je vais en tirer la poussière de votre nez pour vous.\",\n",
       "  'fr': 'Pierre, the abatteur, if you look like that, I will take the sleeve of your nose to you.'},\n",
       " {'en': \"Il pouvait numériser les champs dans toutes les directions, et il pouvait dire combien d'arbres il y avait dans le tremblement le plus lointain, mais de toutes les vues que son jardin, ou que le pays ou le royaume pouvait boire, aucune ne devait être comparée à la perspective de Rosings, offerte par une ouverture dans les arbres qui bordaient le parc près de la s'en en arbres,\",\n",
       "  'fr': \"He could count the fields in all directions, and he could count how many trees he had in the tremblement the most far away, but from all the views that his garden, or that the country or the kingdom could be bounded, none should be compared to Rosings's perspective, offered by a opening in the trees which borded the park near the sand.\"},\n",
       " {'en': 'Son premier devoir était la discrétion; si bien il comprenait.',\n",
       "  'fr': 'His first thought was the discretion; if he was satisfied.'},\n",
       " {'en': \"Et il était étrange de le voir, en chemises, en pantalons courts et en souliers boueux, manipuler cette taillée d'un marquis.\",\n",
       "  'fr': 'And he was astonished at seeing him, in chemises, in trousers and bouils, manipulating this tail of a marquis.'}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in test_dataset[\"input_ids\"]:\n",
    "    print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[363, 4897, 21, 8, 2714, 8158, 113, 133, 43, 10735, 15, 26, 8, 296, 16, 423, 406, 1631, 24, 3, 88, 16415, 16, 66, 8, 12540, 38, 2164, 38, 255, 141, 3883, 16, 48, 798, 16, 3, 9, 712, 388, 55, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[37, 9467, 47, 11743, 120, 3, 7, 40, 3868, 45, 8, 3, 60, 7, 9, 109, 13, 8, 8882, 17, 7, 6, 11, 62, 130, 1116, 1107, 12, 8, 307, 11, 3731, 13281, 10785, 7, 6, 490, 15941, 8901, 7, 84, 2225, 178, 12, 3, 7, 40, 265, 385, 12, 385, 6, 68, 3, 88, 4728, 12, 8, 2214, 13, 175, 3, 18, 975, 122, 17551, 144, 7, 6, 84, 150, 3, 75, 23, 297, 3, 7361, 344, 135, 6, 11, 8, 2418, 3, 25951, 147, 135, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[299, 1919, 141, 3, 4933, 3737, 8, 13608, 6, 84, 1919, 255, 1114, 437, 160, 166, 3, 102, 6631, 6, 11, 42, 255, 3, 7, 144, 16, 2641, 477, 13, 160, 7985, 6, 3, 7603, 122, 1841, 13, 3, 9, 4897, 6, 8757, 1054, 66, 8, 10680, 5575, 4664, 13, 8, 97, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[27889, 3, 7, 22, 35, 89, 76, 23, 52, 185, 14294, 159, 93, 3351, 29, 3167, 6, 3, 15, 17, 4399, 3, 40, 22, 12982, 154, 20, 2788, 18, 29838, 20, 3351, 7, 5, 1168, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[3, 10857, 546, 22, 173, 260, 521, 155, 6, 3, 173, 3, 7, 22, 35, 3, 60, 324, 9, 155, 759, 18, 7170, 50, 3, 7, 9, 1999, 546, 22, 173, 3, 7, 22, 35, 89, 76, 155, 3, 3496, 13889, 297, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[216, 47, 3, 179, 12, 3, 10942, 2615, 6398, 13, 8, 659, 6, 3, 88, 3, 7, 144, 16, 8, 23289, 4865, 117, 163, 6, 42, 8, 861, 141, 59, 3, 25726, 28, 112, 3, 7, 29, 127, 53, 30890, 6, 3, 88, 228, 59, 9116, 406, 3, 14390, 112, 724, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[37, 29, 6, 4698, 9416, 8, 2249, 6, 16, 3, 9, 360, 1234, 6, 3, 88, 243, 112, 625, 5109, 15, 13, 8072, 2947, 485, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[216, 3, 7, 144, 323, 44, 112, 11581, 6, 11, 974, 41, 532, 21380, 7, 3, 2771, 63, 26, 24, 3, 88, 141, 112, 336, 26838, 61, 10, 96, 4135, 25, 214, 140, 58, 27, 56, 341, 36, 3, 7, 9, 11515, 4720, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[86, 2827, 6, 69, 4523, 9, 26, 2169, 1462, 228, 453, 3, 9, 215, 18, 2961, 1059, 6, 406, 237, 3, 7, 29, 14547, 53, 30, 112, 3, 23604, 55, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[366, 27, 18606, 6, 27, 1622, 3, 9, 3, 7, 17, 99, 697, 6, 68, 150, 2107, 42, 731, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[4872, 3, 7, 232, 7, 3, 7, 8130, 8, 2981, 590, 8, 38, 75, 35, 7, 23814, 7, 6, 11, 3, 88, 4728, 12, 19556, 8, 3, 10913, 7, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[3, 26, 22, 7754, 9, 11260, 29, 47, 3, 179, 12, 36, 14249, 6, 437, 6, 13, 112, 192, 14645, 6, 80, 47, 3, 17, 76, 15, 11, 8, 119, 47, 3, 1621, 76, 26, 53, 12, 112, 3984, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[27, 47, 78, 28913, 15, 8549, 26, 44, 48, 903, 24, 255, 141, 14840, 21, 128, 97, 8, 800, 13, 82, 293, 5129, 7, 10, 66, 82, 3, 9, 102, 22459, 106, 7, 130, 3, 17378, 365, 8, 5709, 7, 24, 27, 141, 3, 9, 3, 1625, 3, 9, 969, 526, 13, 16, 8377, 9, 109, 14506, 485, 11, 3, 9, 21315, 1238, 13, 3, 9, 817, 15, 26644, 13, 8, 936, 1405, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[432, 106, 7, 6, 18763, 6, 3, 60, 18, 2248, 17, 76, 15, 8, 388, 227, 8, 54, 29, 106, 55, 27, 3011, 535, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[3625, 11982, 6, 8, 511, 6, 4760, 112, 6026, 30, 8, 160, 346, 10, 3, 88, 141, 14057, 112, 819, 5, 1]] [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[1615, 58, 1]] [[1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_dataset)):\n",
    "    print(test_dataset[\"input_ids\"][i], test_dataset[\"attention_mask\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
